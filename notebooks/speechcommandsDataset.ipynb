{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "261692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModel\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.evaluation.representations import compute_linear_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45f19ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.26G/2.26G [00:54<00:00, 44.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/speechcommands\"\n",
    "\n",
    "# Load the full dataset (v0.02 with 35 classes)\n",
    "import torchaudio\n",
    "dataset = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "    root=data_folder,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    word_dict = {\n",
    "        \"yes\": 0,\n",
    "        \"no\": 1,\n",
    "        \"up\": 2,\n",
    "        \"down\": 3,\n",
    "        \"left\": 4,\n",
    "    }\n",
    "    \"\"\"\n",
    "    Processes a batch of samples to make waveforms uniform in length.\n",
    "    A batch is a list of tuples: (waveform, sample_rate, label, speaker_id, utterance_number)\n",
    "    \"\"\"\n",
    "    target_length = 16000  # 1 second at 16kHz\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each sample in the batch\n",
    "    for waveform, _, label, _, _ in batch:\n",
    "        # Pad the waveform if it's shorter than the target length\n",
    "        if waveform.shape[1] < target_length:\n",
    "            padding_needed = target_length - waveform.shape[1]\n",
    "            padding = torch.zeros((1, padding_needed))\n",
    "            waveform = torch.cat([waveform, padding], dim=1)\n",
    "        # Truncate the waveform if it's longer\n",
    "        else:\n",
    "            waveform = waveform[:, :target_length]\n",
    "        \n",
    "\n",
    "        waveforms.append(waveform)\n",
    "        labels.append(label) # You'll likely want to convert these strings to numbers later\n",
    "\n",
    "    # Stack the processed waveforms into a single tensor\n",
    "    waveforms_tensor = torch.stack(waveforms)\n",
    "    \n",
    "    return waveforms_tensor.squeeze(1), labels\n",
    "\n",
    "# When creating your DataLoader, pass your custom function\n",
    "# Assuming 'dataset' is your torchaudio.datasets.SPEECHCOMMANDS object\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=100, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn  # This is the key part\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7095102f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SPEECHCOMMANDS' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\u001b[38;5;241m.\u001b[39mlabels\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SPEECHCOMMANDS' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15f53942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexkaravos/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/alexkaravos/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/Users/alexkaravos/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1059 [00:00<?, ?it/s]/Users/alexkaravos/anaconda3/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1059/1059 [04:45<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "device = 'mps'\n",
    "model.to(device)\n",
    "labels = []\n",
    "representations = []\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        waveform, label = batch\n",
    "        \n",
    "        labels += label\n",
    "        representations.append(model(waveform.to(device)).last_hidden_state.cpu())\n",
    "representations = torch.cat(representations, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ef8c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mean,feat_std = representations.mean(dim=1),representations.std(dim=1)\n",
    "cat_feat = torch.cat([feat_mean,feat_std],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06c23b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_labels = [\n",
    "    'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "    'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "    'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree',\n",
    "    'two', 'up', 'visual', 'wow', 'yes', 'zero'\n",
    "]\n",
    "int_labels = [speech_labels.index(label) for label in labels]\n",
    "int_labels = torch.tensor(int_labels).int()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d2a6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(int_labels)\n",
    "train_len = int(dataset_len * 0.8)\n",
    "test_len = int(dataset_len * 0.2)\n",
    "\n",
    "idx = torch.randperm(dataset_len)\n",
    "train_idx = idx[:train_len]\n",
    "test_idx = idx[train_len:]\n",
    "\n",
    "train_representations = feat_mean[train_idx]\n",
    "test_representations = feat_mean[test_idx]\n",
    "train_labels = int_labels[train_idx]\n",
    "test_labels = int_labels[test_idx]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_representations, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_representations, test_labels)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aefd9faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21166"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8f405c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 85/85 [00:00<00:00, 138.89it/s, Loss=2.6595]\n",
      "Epoch 2: 100%|██████████| 85/85 [00:00<00:00, 87.54it/s, Loss=2.2478] \n",
      "Epoch 3: 100%|██████████| 85/85 [00:00<00:00, 116.67it/s, Loss=1.9485]\n",
      "Epoch 4: 100%|██████████| 85/85 [00:00<00:00, 109.93it/s, Loss=1.7662]\n",
      "Epoch 5: 100%|██████████| 85/85 [00:00<00:00, 120.22it/s, Loss=1.6011]\n",
      "Epoch 6: 100%|██████████| 85/85 [00:00<00:00, 91.33it/s, Loss=1.5108]\n",
      "Epoch 7: 100%|██████████| 85/85 [00:00<00:00, 120.11it/s, Loss=1.3742]\n",
      "Epoch 8: 100%|██████████| 85/85 [00:00<00:00, 119.63it/s, Loss=1.3044]\n",
      "Epoch 9: 100%|██████████| 85/85 [00:00<00:00, 118.14it/s, Loss=1.1483]\n",
      "Epoch 10: 100%|██████████| 85/85 [00:00<00:00, 87.47it/s, Loss=1.1267]\n",
      "Epoch 11: 100%|██████████| 85/85 [00:00<00:00, 115.16it/s, Loss=1.1384]\n",
      "Epoch 12: 100%|██████████| 85/85 [00:00<00:00, 117.58it/s, Loss=1.0657]\n",
      "Epoch 13: 100%|██████████| 85/85 [00:00<00:00, 117.97it/s, Loss=1.0502]\n",
      "Epoch 14: 100%|██████████| 85/85 [00:00<00:00, 85.33it/s, Loss=1.0164]\n",
      "Epoch 15: 100%|██████████| 85/85 [00:00<00:00, 105.02it/s, Loss=0.9724]\n",
      "Epoch 16: 100%|██████████| 85/85 [00:00<00:00, 120.29it/s, Loss=0.9577]\n",
      "Epoch 17: 100%|██████████| 85/85 [00:00<00:00, 113.87it/s, Loss=0.9371]\n",
      "Epoch 18: 100%|██████████| 85/85 [00:00<00:00, 91.20it/s, Loss=0.8457]\n",
      "Epoch 19: 100%|██████████| 85/85 [00:00<00:00, 107.15it/s, Loss=0.8879]\n",
      "Epoch 20: 100%|██████████| 85/85 [00:00<00:00, 114.60it/s, Loss=0.9398]\n",
      "Epoch 21: 100%|██████████| 85/85 [00:00<00:00, 121.05it/s, Loss=0.8619]\n",
      "Epoch 22: 100%|██████████| 85/85 [00:00<00:00, 88.95it/s, Loss=0.8301]\n",
      "Epoch 23: 100%|██████████| 85/85 [00:00<00:00, 119.16it/s, Loss=0.7380]\n",
      "Epoch 24: 100%|██████████| 85/85 [00:00<00:00, 116.88it/s, Loss=0.7877]\n",
      "Epoch 25: 100%|██████████| 85/85 [00:00<00:00, 116.75it/s, Loss=0.8014]\n",
      "Epoch 26: 100%|██████████| 85/85 [00:00<00:00, 85.75it/s, Loss=0.7200] \n",
      "Epoch 27: 100%|██████████| 85/85 [00:00<00:00, 113.59it/s, Loss=0.8208]\n",
      "Epoch 28: 100%|██████████| 85/85 [00:00<00:00, 117.97it/s, Loss=0.7634]\n",
      "Epoch 29: 100%|██████████| 85/85 [00:00<00:00, 116.98it/s, Loss=0.6968]\n",
      "Epoch 30: 100%|██████████| 85/85 [00:00<00:00, 115.83it/s, Loss=0.6889]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self,input_dim,num_classes):\n",
    "        super(LinearClassifier,self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self,input_dim,num_classes):\n",
    "        super(AttentionClassifier,self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1536,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (bs, 49, 768)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Expand cls_token for batch\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (bs, 1, 768)\n",
    "        \n",
    "        # Use input sequence as memory for decoder\n",
    "        memory = x  # (bs, 49, 768)\n",
    "        \n",
    "        # Pass cls_token through transformer decoder\n",
    "        decoded = self.transformer_decoder(cls_tokens, memory)  # (bs, 1, 768)\n",
    "        \n",
    "        # Extract cls token and classify\n",
    "        cls_output = decoded.squeeze(1)  # (bs, 768)\n",
    "        logits = self.classifier(cls_output)  # (bs, 35)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = LinearClassifier(768,35)\n",
    "model.to('mps')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(30):\n",
    "    lossval = 0   \n",
    "    epoch_bar = tqdm(train_dataloader,desc=f'Epoch {epoch+1}')  \n",
    "    for batch in epoch_bar:\n",
    "        features, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features.to('mps'))\n",
    "        loss = criterion(outputs, labels.to('mps'))\n",
    "        loss.backward()\n",
    "        lossval += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "10c69463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        features, label = batch\n",
    "        \n",
    "        predictions.append(model(features.to('mps')).cpu().argmax(dim=1))\n",
    "        labels.append(label)\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "accuracy = (predictions == labels).float().mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "898544a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12074510112527835\n"
     ]
    }
   ],
   "source": [
    "#get the pca of the cat features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(cat_feat)\n",
    "pca_feat = pca.transform(cat_feat)\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=35,random_state=0)\n",
    "kmeans.fit(pca_feat)\n",
    "\n",
    "print(normalized_mutual_info_score(kmeans.labels_,int_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7995883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
